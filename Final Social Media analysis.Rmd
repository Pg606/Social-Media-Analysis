---
title: "Final Analysis"
author: "pg606@scarletmail.rutgers.edu"
date: "2024-04-29"
output: html_document
---

```{r }
library(MASS)
library(ggplot2)
library(memisc)
library(ROCR)
library(dplyr)
library(klaR)
library(memisc)
library(ggplot2)
library(corrplot)
library(factoextra)
library(psych)
library(NbClust)
library(caTools)
library(magrittr)
library(pROC)
library(caret)


sc_dataset <- read.csv("/Users/Prateekg/Documents/MVA/Midterm_new.csv", row.names=1)
str(sc_dataset)
correlation_matrix <- cor(sc_dataset[,1:11])
correlation_matrix

corrplot(cor(sc_dataset), type = "upper", method = "color")

```

• The correlation matrix shows us a correlation between the columns in both cases. 

• Hence, Principal Component Analysis (PCA) can be used to reduce the number of columns for the analysis. 

```{r }

# PCA

sc_dataset_pca <- prcomp(sc_dataset[,1:8],scale=TRUE) 
sc_dataset_pca

summary(sc_dataset_pca)

fviz_eig(sc_dataset_pca, addlabels = TRUE)
#plot(sc_dataset, xlab = "Component number", ylab = "Component variance", main = "Scree diagram")
```

• The scree diagram shows us that sum of the first 2 principal components is less than 70%.

• So, we cannot move forward using PCA for column reduction. 

• We now move on to check EFA for this dataset.

# How do the usage patterns of social media platforms (Instagram, LinkedIn, Snapchat, Twitter, Whatsapp, Youtube, OTT, and Reddit) among individuals in the dataset relate to their reported health status regarding trouble falling asleep



```{r}

fviz_pca_var(sc_dataset_pca,col.var = "cos2",
             gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"),
             repel = TRUE)
```


```{r}


# EFA


fit.pc <- principal(sc_dataset[,1:8], nfactors=4, rotate="varimax") 
fit.pc



fa.diagram(fit.pc) 

vss(sc_dataset[,1:8])

```

Defining the factors Obtained-
RC1

• Both Twitter and Reddit serve as platforms for social networking, allowing users to connect with others, share content, and engage in discussions.

RC2 

• Snapchat, LinkedIn, Instagram are popular all over the world. 

• LinkedIn is used for professional purposes. Snapchat is used for chatting and 
Instagram is used for posting photos and videos. 

RC3

• Whatsapp & OTT are famous all over the world. 

• Both are used for chatting and entertainment.

RC4

• Youtube is used for both learning and entertainment. 


```{r}
# CLUSTERING

efa_data <- as.data.frame(fit.pc$scores)
efa_data


matstd_sm <- scale(efa_data)



res.hc <- matstd_sm %>% scale() %>% dist(method = "euclidean") %>%
  hclust(method = "ward.D2")

res.nbclust <- matstd_sm %>% scale() %>% NbClust(distance = "euclidean", min.nc = 3, max.nc = 13, method = "complete", index ="all") 

#fviz_nbclust(res.nbclust, ggtheme = theme_minimal())

km.res <- kmeans(matstd_sm, 5, nstart = 10)

fviz_cluster(km.res, data = matstd_sm,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())

dist.sc_dataset <- dist(matstd_sm, method="euclidean")



fviz_dend(res.hc, k = 5, # Cut in four groups
          cex = 0.5, # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
)


Clustered <- ifelse(km.res$cluster > 1.5, "Not Healthy", "Healthy")
Actual <- ifelse(sc_dataset$Trouble_falling_asleep == 1, "Healthy", "Not Healthy")
confusion_mat <- table(Clustered, Actual)
confusion_mat

accuracy <- sum(diag(confusion_mat)) / sum(confusion_mat)
precision <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
recall <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
cat("Accuracy:", round(accuracy, 3), "\n")
## Accuracy: 0.623
cat("Precision:", round(precision, 3), "\n")
## Precision: 0.16
cat("Recall:", round(recall, 3), "\n")




```

• We can see that the confusion matrix shows the clustering is done in a way where almost all the users are not feeling any trouble in falling asleep .

• This shows that we cannot classify our data into trouble falling asleep yes or no based on the variables given.

• The precision is obtained to be 92% which is not so bad.


```{r}
wdbc <- read.csv("/Users/Prateekg/Documents/MVA/Midterm_new.csv")
features <- c("Instagram", "LinkedIn", "Snapchat", "Twitter", "Whatsapp", "Youtube", "OTT", "Reddit", "Trouble_sleep", "Mood", "Tired_morning")
names(wdbc) <- c("ID", "Instagram", "LinkedIn", "Snapchat", "Twitter", "Whatsapp", "Youtube", "OTT", "Reddit", "Trouble_sleep", "Mood", "Tired_morning")
wdbc<- wdbc
wdbc.data <- as.matrix(wdbc[,c(2:9)])
row.names(wdbc.data) <- wdbc$ID
wdbc_raw <- cbind(wdbc.data, as.numeric(as.factor(wdbc$Trouble_sleep))-1)
colnames(wdbc_raw)[9] <- "TroubleInSleep"
smp_size_raw <- floor(0.70 * nrow(wdbc_raw))
train_ind_raw <- sample(nrow(wdbc_raw), size = smp_size_raw)
train_raw.df <- as.data.frame(wdbc_raw[train_ind_raw, ])
test_raw.df <- as.data.frame(wdbc_raw[-train_ind_raw, ])

```


Now divided the dataset into training and testing data for regression analysis.
We consider the training and testing data with a 70-30 split.


```{r}
# MULTIPLE REGRESSION
#fit <- lm(TroubleInSleep~Instagram + Snapchat + Twitter, data=sc_dataset) 
x_sm <- cbind(train_ind_raw,train_raw.df[, -9])

fit <- lm(TroubleInSleep ~ ., data = train_raw.df) 

summary(fit)
coefficients(fit)
residuals(fit)
probabilities_sm2 <- predict(fit, newdata = test_raw.df, type = "response")

predicted_sm2 <- ifelse(probabilities_sm2 > 0.5, "Yes", "No")
actual_sm <- ifelse(test_raw.df$TroubleInSleep == 1, "Yes", "No")
confusion_sm2 <- table(predicted_sm2, actual_sm)
confusion_sm2

roc_sm <- roc(test_raw.df$TroubleInSleep, probabilities_sm2)
auc_sm <- auc(roc_sm)

ggroc(roc_sm, color = "blue", legacy.axes = TRUE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  scale_x_continuous(labels = scales::percent_format()) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "False Positive Rate", y = "True Positive Rate",
       title = paste("ROC Curve (AUC = ", round(auc_sm, 2), ")")) +
  annotate("text", x = 0.5, y = 0.5, label = paste0("AUC = ", round(auc_sm, 2)))

threshold<-0.5
predicted <- predict(fit, newdata = test_raw.df)

predicted_category <- ifelse(predicted > threshold, "No Trouble in Sleep", "Trouble in sleep")

# Convert actual values to binary categories
actual_category <- ifelse(test_raw.df$TroubleInSleep == 1, "No Trouble in sleep", "Trouble in sleep")

# Create a confusion matrix
confusion_mat <- table(predicted_category, actual_category)

# Calculate accuracy
accuracy <- sum(diag(confusion_mat)) / sum(confusion_mat)
accuracy

```

The regression summary shows that we have significant variables that affect the output variable.
For multiple regression we got the accuracy as approx 42.8 %
AUC of ROC curve is 80%

We can now check how the logistic regression-

```{r}
# Logistic Regression

logistic_sm <- glm(TroubleInSleep ~ ., data = train_raw.df, family = 'binomial')
summary(logistic_sm)

predicted.data <- data.frame(probability.of.hd=logistic_sm$fitted.values,TroubleInSleep=train_raw.df$TroubleInSleep)
predicted.data <- predicted.data[order(predicted.data$probability.of.hd, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)

ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
  geom_point(aes(color=TroubleInSleep), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of getting in sleeping")
probabilities_sm2 <- predict(logistic_sm, newdata = test_raw.df, type = "response")

predicted_sm2 <- ifelse(probabilities_sm2 > 0.5, "Yes", "No")
actual_sm <- ifelse(test_raw.df$TroubleInSleep == 1, "Yes", "No")
confusion_sm2 <- table(predicted_sm2, actual_sm)
confusion_sm2
accuracy2 <- sum(diag(confusion_sm2)) / sum(confusion_sm2)
precision2 <- confusion_sm2[2, 2] / sum(confusion_sm2[, 2])
recall2 <- confusion_sm2[2, 2] / sum(confusion_sm2[2, ])
cat("Accuracy:", round(accuracy2, 3), "\n")
cat("Precision:", round(precision2, 3), "\n")
cat("Recall:", round(recall2, 3), "\n")
probabilities <- predict(logistic_sm, newdata = test_raw.df, type = "response")

roc_sm <- roc(test_raw.df$TroubleInSleep, probabilities_sm2)
auc_sm <- auc(roc_sm)

ggroc(roc_sm, color = "blue", legacy.axes = TRUE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  scale_x_continuous(labels = scales::percent_format()) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "False Positive Rate", y = "True Positive Rate",
       title = paste("ROC Curve (AUC = ", round(auc_sm, 2), ")")) +
  annotate("text", x = 0.5, y = 0.5, label = paste0("AUC = ", round(auc_sm, 2)))
```

In the logistic regression, getting the accuracy as 82.9%.

AUC of ROC curve = 60%



```{r}

# DISCRIMINANT MODEL
wdbc_raw.lda <- lda(formula = TroubleInSleep ~ ., data = train_raw.df)
wdbc_raw.lda
summary(wdbc_raw.lda)
plot(wdbc_raw.lda)
predictions <- predict(wdbc_raw.lda, newdata = test_raw.df)
predicted_classes <- predictions$class
accuracy2 <- mean(predicted_classes == test_raw.df$TroubleInSleep)
accuracy2
wdbc_raw.lda.predict_train <- predict(wdbc_raw.lda, newdata = train_raw.df)
y<-wdbc_raw.lda.predict_train$class
wdbc_raw.lda.predict_train$x
table(y,train_raw.df$TroubleInSleep)
wdbc_raw.lda.predict_test <- predict(wdbc_raw.lda, newdata = test_raw.df)
y<-wdbc_raw.lda.predict_test$class
wdbc_raw.lda.predict_test$x
table(y,test_raw.df$TroubleInSleep)
wdbc_raw.lda.predict.posteriors <- as.data.frame(wdbc_raw.lda.predict_test$posterior)
pred <- prediction(wdbc_raw.lda.predict.posteriors[,2], test_raw.df$TroubleInSleep)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
plot(wdbc_raw.lda, col = as.integer(train_raw.df$TroubleInSleep))

  

```

The accuracy for this model is 42.8%

The AUC of ROC curve = 60%

Regression Summary-
We can see that we can predict if the student is getting affected or student is facing problem while falling asleep or not based on the time they have spent on the individual social media apps.

Answer to our question 2: Yes, we can predict.


